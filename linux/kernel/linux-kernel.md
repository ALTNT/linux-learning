# linux-kernel

理解Linux内核是一项复杂而深刻的任务，因为它涉及到操作系统的核心功能。以下是对你提到的几个关键方面的详细解释：

1. 进程调度：

   - 调度器： Linux内核使用调度器来管理系统中运行的进程。CFS（完全公平调度器）是Linux中常用的调度器之一，它旨在实现对处理器的公平分配。
   - 进程状态： 进程可以处于运行、就绪、阻塞等状态。调度器的任务是按照一定的算法从就绪队列中选择一个进程并将其分配给CPU执行。

2. 同步机制：

   - 原子操作： Linux内核提供原子操作来确保某些操作的不可分割性，以防止多个进程同时执行这些操作。
   - 信号量和互斥体： 用于控制对共享资源的访问，以避免竞态条件和数据损坏。

3. 内存管理：

   - 虚拟内存： Linux使用虚拟内存来扩展可用的物理内存。这涉及到分页和分段机制，使得每个进程都有自己的虚拟地址空间。
   - 内存映射和页面置换： Linux内核使用内存映射来管理进程的虚拟地址空间，而页面置换策略确保物理内存的有效使用。

4. 中断系统：

   - 中断处理： 中断是硬件或软件引起的事件，需要立即处理。Linux内核有中断处理程序来响应中断事件。
   - 中断向量表： 内核维护一个中断向量表，将中断号映射到相应的中断服务例程，以执行特定的处理。

5. Linux操作系统整体框架和原理：

   - 模块化设计： Linux内核采用模块化设计，允许动态加载和卸载内核模块，以减小内核的大小并提高灵活性。
   - 文件系统： Linux支持多种文件系统，如EXT4、Btrfs等，提供对磁盘上数据的组织和管理。

   - 系统调用： 用户空间程序通过系统调用与内核进行通信，请求执行特权操作。

在应用程序开发、驱动开发、网络通信协议开发、性能调优、中间件和虚拟化等领域，对Linux内核的深入理解是至关重要的。

- 例如，在驱动开发中，你可能需要理解设备驱动模型、I/O 系统和中断处理；
- 在网络通信协议开发中，你需要了解套接字编程和网络协议栈；
- 在性能调优中，你需要深入了解系统的性能瓶颈并优化相关参数。对于中间件和虚拟化，你需要了解相关的技术和实现原理。

## 进程调度

Linux操作系统采用多种调度器，其中最常见的包括：

1. CFS（完全公平调度器）：

   - CFS是Linux内核中默认的调度器，旨在实现对处理器的公平分配。它使用红黑树来维护就绪队列，通过动态地调整进程的虚拟运行时间来分配CPU时间片，以实现公平性。

2. O(1)调度器：

   - O(1)调度器是早期版本中使用的调度器，它的目标是以常量时间复杂度执行调度。但随着系统的复杂性增加，它在某些方面的性能表现不如CFS，因此逐渐被淘汰。

3. 实时调度器（RT）：

   - 针对实时任务的需求，Linux内核提供了实时调度器。其中最常用的是SCHED_FIFO和SCHED_RR。SCHED_FIFO是先来先服务调度，而SCHED_RR是时间片轮转调度，允许任务在一个时间片内运行。

4. Deadline调度器：

   - Deadline调度器是一种实时调度器，它引入了截止时间的概念，以确保任务在截止时间内完成。这对于一些实时系统和嵌入式系统非常重要。

这些调度器之间的选择通常取决于应用程序的需求和系统的特性。大多数情况下，CFS是默认选择，因为它在大多数工作负载下表现良好，并且更加注重公平性。如果系统有实时任务的需求，可以选择实时调度器或Deadline调度器。

### 红黑树

红黑树是一种自平衡的二叉搜索树，它具有以下特性：

1. 时间复杂度：
   - 查找（Search）： O(log n)
   - 插入（Insert）： O(log n)
   - 删除（Delete）： O(log n)

    这里的 n 表示红黑树中节点的数量。由于红黑树是自平衡的，其高度受到对数级别的限制，因此这些操作的时间复杂度是对数级别的。

2. 空间复杂度：
   - 空间复杂度主要由节点本身的存储和额外的颜色信息组成。
   - 每个节点通常需要存储指向左右子节点的指针、键值（或数据）、父节点指针、以及表示节点颜色的额外信息。这些信息的空间复杂度是 O(1)。
   - 因此，红黑树的总体空间复杂度是 O(n)，其中 n 是树中节点的数量。

需要注意的是，虽然红黑树的常规操作（查找、插入、删除）的平均时间复杂度是 O(log n)，但在最坏情况下的时间复杂度也是 O(log n)。这是由于红黑树的自平衡性质，使得树的高度保持在对数级别。

## 内核互斥技术

在内核中，可能出现多个进程（通过系统调用进入内核模式）访问同一个对象、进程
和硬中断访问同一个对象、进程和软中断访问同一个对象、多个处理器访问同一个对象等
现象，我们需要使用互斥技术，确保在给定的时刻只有一个主体可以进入临界区访问对象。

- 如果临界区的执行时间比较长或者可能睡眠，可以使用下面这些互斥技术。
  - （1）信号量，大多数情况下我们使用互斥信号量。
  - （2）读写信号量。
  - （3）互斥锁。
  - （4）实时互斥锁。

  申请这些锁的时候，如果锁被其他进程占有，进程将会睡眠等待，代价很高。

- 如果临界区的执行时间很短，并且不会睡眠，那么使用上面的锁不太合适，因为进程
切换的代价很高，可以使用下面这些互斥技术。
  - （1）原子变量。
  - （2）自旋锁。
  - （3）读写自旋锁，它是对自旋锁的改进，允许多个读者同时进入临界区。
  - （4）顺序锁，它是对读写自旋锁的改进，读者不会阻塞写者。

  申请这些锁的时候，如果锁被其他进程占有，进程自旋等待（也称为忙等待）。

- 进程还可以使用下面的互斥技术。
  - （1）禁止内核抢占，防止被当前处理器上的其他进程抢占，实现和当前处理器上的其
  他进程互斥。
  - （2）禁止软中断，防止被当前处理器上的软中断抢占，实现和当前处理器上的软中断
  互斥。
  - （3）禁止硬中断，防止被当前处理器上的硬中断抢占，实现和当前处理器上的硬中断
  互斥。

- 在多处理器系统中，为了提高程序的性能，需要尽量减少处理器之间的互斥，使处理
器可以最大限度地并行执行。从互斥信号量到读写信号量的改进，从自旋锁到读写自旋锁
的改进，允许读者并行访问临界区，提高了并行性能，但是我们还可以进一步提高并行性
能，使用下面这些避免使用锁的互斥技术。
  - （1）每处理器变量。
  - （2）每处理器计数器。
  - （3）内存屏障。
  - （4）读-复制更新（Read-Copy Update，RCU）。
  - （5）可睡眠 RCU。

使用锁保护临界区，如果使用不当，可能出现死锁问题。内核里面的锁非常多，定位
很难，为了方便定位死锁问题，内核提供了死锁检测工具 lockdep。

### 可睡眠RCU与RCU的区别？

RCU（Read-Copy Update）和可睡眠 RCU 是两种用于实现并发读取和更新的无锁同步机制，但它们在一些方面存在区别。

RCU（Read-Copy Update）：

1. 特点：

   - RCU 是一种无锁同步机制，主要用于在多处理器系统中实现高效的读取和更新。
   - 在 RCU 中，读操作是非阻塞的，不会阻塞其他读者，且可以与写者并发进行。
   - 写操作（更新）可能会有一定的延迟，因为它要等待正在执行的读者完成后，再应用更新。

2. 应用场景：

   - RCU 适用于读多写少的场景，其中读操作占主导地位。

可睡眠 RCU：

1. 特点：

   - 可睡眠 RCU 是对传统 RCU 的扩展，允许在临界区执行可能会导致进程睡眠的操作。
   - 传统 RCU 不允许在临界区中执行可能导致进程睡眠的操作，因为这可能会破坏 RCU 的原理。可睡眠 RCU 通过使用睡眠锁和条件变量等技术来解决这个问题。

2. 适用场景：

   - 当需要在 RCU 临界区中执行可能导致睡眠的操作时，可睡眠 RCU 是一种更为灵活的选择。

区别总结：

- 睡眠操作支持： 主要区别在于可睡眠 RCU 允许在 RCU 临界区执行可能导致睡眠的操作，而传统 RCU 不支持这样的操作。
- 复杂性： 由于支持睡眠操作，可睡眠 RCU 的实现可能相对复杂一些。
选择使用哪种形式的 RCU 取决于具体的应用场景。如果在临界区执行的操作不涉及可能导致睡眠的情况，传统 RCU 可能更为简单且高效。但如果需要在临界区执行可能导致睡眠的操作，可睡眠 RCU 提供了相应的支持。

## 内存管理

内存管理子系统的架构如图 3.1 所示，分为用户空间、内核空间和硬件 3 个层面。

![内存管理架构图](https://cdn.jsdelivr.net/gh/realwujing/picture-bed/20231122154606.png)

### 1. 用户空间

应用程序使用 malloc()申请内存，使用 free()释放内存。

malloc()和 free()是 glibc 库的内存分配器 ptmalloc 提供的接口，ptmalloc 使用系统调用
brk 或 mmap 向内核以页为单位申请内存，然后划分成小内存块分配给应用程序。

用户空间的内存分配器，除了 glibc 库的 ptmalloc，还有谷歌公司的 tcmalloc 和 FreeBSD
的 jemalloc。

### 2. 内核空间

#### （1）内核空间的基本功能

虚拟内存管理负责从进程的虚拟地址空间分配虚拟页，sys_brk 用来扩大或收缩堆，
sys_mmap 用来在内存映射区域分配虚拟页，sys_munmap 用来释放虚拟页。

内核使用延迟分配物理内存的策略，进程第一次访问虚拟页的时候，触发页错误
异常，页错误异常处理程序从页分配器申请物理页，在进程的页表中把虚拟页映射到物
理页。

页分配器负责分配物理页，当前使用的页分配器是伙伴分配器。

内核空间提供了把页划分成小内存块分配的块分配器，提供分配内存的接口 kmalloc()
和释放内存的接口 kfree()，支持 3 种块分配器：SLAB 分配器、SLUB 分配器和 SLOB
分配器。

在内核初始化的过程中，页分配器还没准备好，需要使用临时的引导内存分配器分配
内存。

#### （2）内核空间的扩展功能

不连续页分配器提供了分配内存的接口 vmalloc 和释放内存的接口 vfree，在内存碎片
化的时候，申请连续物理页的成功率很低，可以申请不连续的物理页，映射到连续的虚拟
页，即虚拟地址连续而物理地址不连续。

每处理器内存分配器用来为每处理器变量分配内存。

连续内存分配器（Contiguous Memory Allocator，CMA）用来给驱动程序预留一段连续
的内存，当驱动程序不用的时候，可以给进程使用；当驱动程序需要使用的时候，把进程
占用的内存通过回收或迁移的方式让出来，给驱动程序使用。

内存控制组用来控制进程占用的内存资源。

当内存碎片化的时候，找不到连续的物理页，内存碎片整理（“memory compaction”
的意译，直译为“内存紧缩”）通过迁移的方式得到连续的物理页。

在内存不足的时候，页回收负责回收物理页，对于没有后备存储设备支持的匿名页，
把数据换出到交换区，然后释放物理页；对于有后备存储设备支持的文件页，把数据写回
存储设备，然后释放物理页。如果页回收失败，使用最后一招：内存耗尽杀手（OOM killer，
Out-of-Memory killer），选择进程杀掉。

### 3．硬件层面

处理器包含一个称为内存管理单元（Memory Management Unit，MMU）的部件，负责
把虚拟地址转换成物理地址。
内存管理单元包含一个称为页表缓存（Translation Lookaside Buffer，TLB）的部件，
保存最近使用过的页表映射，避免每次把虚拟地址转换成物理地址都需要查询内存中的
页表。
为了解决处理器的执行速度和内存的访问速度不匹配的问题，在处理器和内存之间增
加了缓存。缓存通常分为一级缓存和二级缓存，为了支持并行地取指令和取数据，一级缓
存分为数据缓存和指令缓存。

### 块分配器

块分配器： 内核提供了块分配器，支持三种：SLAB 分配器、SLUB 分配器和 SLOB 分配器。

SLUB 分配器继承了 SLAB 分配器的核心思想，在某些地方做了改进。

- （1）SLAB 分配器的管理数据结构开销大，早期每个 slab 有一个描述符和跟在后面的
空闲对象数组。SLUB 分配器把 slab 的管理信息保存在 page 结构体中，使用联合体重用 page
结构体的成员，没有使 page 结构体的大小增加。现在 SLAB 分配器反过来向 SLUB 分配器
学习，抛弃了 slab 描述符，把 slab 的管理信息保存在 page 结构体中。
- （2）SLAB 分配器的链表多，分为空闲 slab 链表、部分空闲 slab 链表和满 slab 链表，
管理复杂。SLUB 分配器只保留部分空闲 slab 链表。
- （3）SLAB 分配器对 NUMA 系统的支持复杂，每个内存节点有共享数组缓存和远程节
点数组缓存，对象在这些数组缓存之间转移，实现复杂。SLUB 分配器做了简化。
- （4）SLUB 分配器抛弃了效果不明显的 slab 着色。

SLOB 分配器最大的特点就是简洁，代码只有 600 多行，特别适合小内存的嵌入式设备。

#### SLUB 分配器内存缓存的数据结构

##### 1．数据结构

SLUB 分配器内存缓存的数据结构如图 3.37 所示。

- （1）每个内存缓存对应一个 kmem_cache 实例。

  - 成员 size 是包括元数据的对象长度，成员 object_size 是对象原始长度。

  - 成员 oo 存放最优 slab 的阶数和对象数，低 16 位是对象数，高 16 位是 slab 的阶数，
  即 oo 等于（（slab 的阶数 << 16）| 对象数）。最优 slab 是剩余部分最小的 slab。

  - 成员 min 存放最小 slab 的阶数和对象数，格式和 oo 相同。最小 slab 只需要足够存放
  一个对象。当设备长时间运行以后，内存碎片化，分配连续物理页很难成功，如果分配最
  优 slab 失败，就分配最小 slab。

- （2）每个内存节点对应一个 kmem_cache_node 实例。

  - 链表 partial 把部分空闲的 slab 链接起来，成员 nr_partial 是部分空闲 slab 的数量。

- （3）每个 slab 由一个或多个连续的物理页组成，页的阶数是最优 slab 或最小 slab 的阶
数，如果阶数大于 0，组成一个复合页。

  - slab 被划分为多个对象，如果 slab 长度不是对象长度的整数倍，尾部有剩余部分。尾
部也可能有保留部分，kmem_cache 实例的成员 reserved 存放保留长度。

  - 在创建内存缓存的时候，如果指定标志位 SLAB_TYPESAFE_BY_RCU，要求使用 RCU
  延迟释放 slab，在调用函数 call_rcu 把释放 slab 的函数加入 RCU 回调函数队列的时候，需
  要提供一个 rcu_head 实例，slab 提供的 rcu_head 实例的位置分两种情况。

    - 1）如果 page 结构体的成员 lru 的长度大于或等于 rcu_head 结构体的长度，那么重用
    成员 lru。

    - 2）如果 page 结构体的成员 lru 的长度小于 rcu_head 结构体的长度，那么必须在 slab
    尾部为 rcu_head 结构体保留空间，保留长度是 rcu_head 结构体的长度。

  - page 结构体的相关成员如下。
    - 1）成员 flags 设置标志位 PG_slab，表示页属于 SLUB 分配器。
    - 2）成员 freelist 指向第一个空闲对象。
    - 3）成员 inuse 表示已分配对象的数量。
    - 4）成员 objects 是对象数量。
    - 5）成员 frozen 表示 slab 是否被冻结在每处理器 slab 缓存中。如果 slab 在每处理器 slab
    缓存中，它处于冻结状态；如果 slab 在内存节点的部分空闲 slab 链表中，它处于解冻状态。
    - 6）成员 lru 作为链表节点加入部分空闲 slab 链表。
    - 7）成员 slab_cache 指向 kmem_cache 实例。

- （4）kmem_cache 实例的成员 cpu_slab 指向 kmem_cache_cpu 实例，每个处理器对应一
个 kmem_cache_cpu 实例，称为每处理器 slab 缓存。

  - SLAB 分配器的每处理器数组缓存以对象为单位，而 SLUB 分配器的每处理器 slab 缓
  存以 slab 为单位。
    - 成员 freelist 指向当前使用的 slab 的空闲对象链表，成员 page 指向当前使用的 slab 对
    应的 page 实例，成员 partial 指向每处理器部分空闲 slab 链表。

对象有两种内存布局，区别是空闲指针的位置不同。

第一种内存布局如图 3.38 所示，空闲指针在红色区域 2 的后面。

## 中断

这段文字概述了第10章《中断与时钟》的主要内容，主要涵盖了中断和定时器在Linux设备驱动编程中的重要性以及相应的处理流程和机制。以下是对每个小节的简要解释：

10.1 节 - 中断和定时器的概念及处理流程：
中断和定时器概念： 介绍中断和定时器的基本概念，以及它们在Linux设备驱动中的作用。
处理流程： 概述中断和定时器的处理流程，强调中断服务程序需要尽量短暂，因为它执行在非进程上下文中。

10.2 节 - Linux中断处理程序的架构和顶半部、底半部之间的关系：
中断处理程序架构： 解释Linux中断处理程序的整体架构，包括中断处理的两个主要部分：顶半部（Top Half）和底半部（Bottom Half）。
顶半部和底半部： 讨论顶半部和底半部之间的关系，强调它们的划分是为了确保中断服务程序的执行时间尽量短。

10.3 节 - Linux中断编程的方法：
中断编程方法： 探讨Linux中断编程的方法，包括中断的申请和释放、中断的使能和屏蔽，以及介绍中断底半部相关的概念，如tasklet、工作队列、软中断机制和threaded_irq。

10.4 节 - 多个设备共享同一个中断号时的中断处理过程：
中断处理过程： 解释当多个设备共享同一个中断号时，Linux中的中断处理过程，包括中断的共享机制和相应的处理策略。

10.5 节和 10.6 节 - 定时器的编程和内核延时的方法：
定时器编程： 介绍Linux设备驱动编程中定时器的编程方法。
内核延时： 探讨在Linux内核中实现延时的方法，强调内核软件定时器最终依赖于时钟中断。

这些主题涵盖了在Linux设备驱动编程中涉及中断和定时器的核心概念、架构和编程方法。理解这些内容对于设计和实现高效、稳定的设备驱动程序至关重要。

### 软中断和tasklet运行于软中断上下文，仍然属于原子上下文的一种，而工作队列则运行于进程上下 文。因此，在软中断和tasklet处理函数中不允许睡眠，而在工作队列处理函数中允许睡眠。

软中断（Softirq）：

运行于软中断上下文，也被认为是一种原子上下文。
在软中断中运行的代码应该是非阻塞的，不允许执行可能导致进程切换（调度）的操作，如休眠。
主要用于在内核中处理底半部工作。
Tasklet：

也运行于软中断上下文，因此同样属于原子上下文。
与软中断类似，tasklet 中的代码应该是非阻塞的。
Tasklet 提供了比软中断更方便的 API，使得处理底半部工作更加简单。
工作队列（Work Queue）：

运行于进程上下文。
工作队列允许执行可能导致进程切换的操作，如休眠。
由于在进程上下文中执行，工作队列更适合执行那些可能耗时较长的任务。
在Linux内核中，软中断和tasklet通常用于处理一些紧急的、轻量级的任务，以避免在顶半部中执行过多的工作。因为它们运行在软中断上下文，所以它们的执行应该是快速的、不可睡眠的。

工作队列则更适合那些不太紧急、允许较长执行时间的任务，因为它们可以在进程上下文中执行，允许进行较长时间的处理，甚至是休眠。

### 进程上下文、线程上下文、中断上下文的区别？

在操作系统中，"上下文"通常指的是执行环境和状态的集合，不同的上下文代表了不同的执行环境。在 Linux 内核中，常见的上下文包括进程上下文、线程上下文和中断上下文。以下是它们的主要区别：

进程上下文（Process Context）：

概念： 进程上下文是指一个运行在内核态的进程的上下文环境。
特点： 在进程上下文中，内核代码执行的是进程的名义上下文，因此可以访问进程的地址空间和所有用户空间数据。
切换代价： 进程上下文切换的代价相对较高，因为需要保存和还原完整的进程状态，包括寄存器、页表等。

线程上下文（Thread Context）：

概念： 线程上下文是指一个运行在内核态的线程的上下文环境。
特点： 与进程上下文相似，线程上下文也可以访问线程的地址空间和用户空间数据。
切换代价： 线程上下文切换的代价相对于进程上下文来说较小，因为在同一地址空间中运行的线程共享了相同的页表。

中断上下文（Interrupt Context）：

概念： 中断上下文是指内核在响应硬件中断时执行的上下文环境。
特点： 中断上下文运行在中断服务例程（ISR）中，它必须迅速完成以便尽早恢复硬件中断。
限制： 中断上下文有一些限制，例如不允许休眠、不允许访问用户空间数据，因为这可能导致不可预测的延迟。
总体而言，这三种上下文反映了不同的执行环境和执行要求。在内核中，合理的使用这些上下文是确保系统稳定性和性能的关键。

### 网卡驱动net-device，suckbf这两个数据结构很关键，驱动中收包逻辑通过中断，这个稍微复杂些

您提到的 "net-device" 和 "skb"（假设 "suckbf" 是打字错误，应该是 "skb"）是 Linux 网络驱动开发中的两个关键数据结构。

net_device：

net_device 是 Linux 网络设备的抽象表示。每个网络设备（例如网卡）都由一个 net_device 结构体表示。
该结构体包含了网络设备的各种信息，如设备名称、MAC 地址、状态等。
驱动程序通过操作 net_device 结构体来控制和管理网络设备。
skb（Socket Buffer）：

skb 是 Linux 内核中用于表示网络数据包的结构体。它包含了传输层和网络层的数据，以及与数据包相关的元数据。
skb 中包含了指向数据区域的指针，以及与数据包相关的一系列头部信息。
skb 是网络驱动中非常关键的数据结构，用于传递和处理网络数据包。
如果您是在谈论网络驱动的中断处理，那么通常是使用中断机制来接收网络数据包。下面是简要的网络驱动中收包逻辑的概述：

中断处理：

网卡通常会生成中断，以通知系统有新的数据包到达。这可以是硬中断或者通过 Polling 机制。
中断服务程序（ISR）是一个与中断相关的函数，它会在中断发生时执行。在网络驱动中，ISR 的任务是通知内核有新的数据包到达。
数据包接收：

ISR 负责从网卡接收数据包，将数据包存储到内存中。
接收到的数据包通常会以 skb 结构体的形式存在。
net_device 处理：

接着，网络驱动会将接收到的数据包封装成 skb，并将 skb 与对应的 net_device 关联起来。
驱动可能会更新 net_device 结构体中的统计信息（如接收的字节数、数据包数量等）。
上层处理：

最后，驱动会将 skb 传递给协议栈的上层，让网络协议栈（例如TCP/IP协议栈）进一步处理。
这是一个简单的概述，实际上，网络驱动的开发涉及到很多复杂的细节，包括对各种网络协议的处理、中断处理程序的高效性等。

### 10.7 总结

Linux的中断处理分为两个半部，顶半部处理紧急的硬件操作，底半部处理不紧急的耗时操作。tasklet 和工作队列都是调度中断底半部的良好机制，tasklet基于软中断实现。内核定时器也依靠软中断实现。 

内核中的延时可以采用忙等待或睡眠等待，为了充分利用CPU资源，使系统有更好的吞吐性能，在对 延迟时间的要求并不是很精确的情况下，睡眠等待通常是值得推荐的，而ndelay（）、udelay（）忙等待 机制在驱动中通常是为了配合硬件上的短时延迟要求。

## 17.1 I2C、SPI、USB驱动架构

您提到的内容涉及Linux中I2C、SPI、USB等总线驱动的架构。下面是对这些内容的一些解释：

总线驱动架构：

Linux的总线驱动架构采用了一种分层的设计，将主机端的驱动与外设端的驱动分离。
通过一个核心层对某种总线的协议进行抽象，外设端的驱动可以通过调用核心层的API间接访问主机端的驱动传输函数。
这种设计使得在不同总线上运行的外设可以共享相同的驱动接口，提高了驱动的可移植性。
板级描述信息：

对于像I2C、SPI这样不具备热插拔能力的总线，通常需要在arch/arm/mach-xxx或者arch/arm/boot/dts中提供板级描述信息。
这些描述信息包括外设与主机的连接情况，有助于内核正确地配置和初始化驱动。
USB、PCI等总线：

USB、PCI等总线具备热插拔能力，因此在板级描述信息中通常不需要详细描述外设与主机的连接情况。
Linux USB子系统和PCI子系统具有自动探测的功能，可以动态地发现和管理连接到系统的外设。
架构类比：

不仅限于I2C、SPI、USB，Linux的各个子系统都遵循相似的总线驱动架构。
表17.1中列举了I2C、SPI、USB的架构类比，说明了它们在整体架构上的相似性。
控制器的枚举：

I2C、SPI、USB控制器实际上也是由它们自身依附的总线枚举出来的。
这些控制器通常直接集成在SoC内部，通过内存访问指令来访问。它们通过platform_driver、platform_device等模型被枚举进内核。
总体而言，这种总线驱动的设计和架构使得Linux内核能够更好地支持不同类型的总线和外设，并且具有良好的可移植性和可扩展性。

## Linux 程序开发常用调试工具合集

Linux 程序开发常用调试工具合集

```text
01 总览
编译阶段
nm                 获取二进制文件包含的符号信息
strings           获取二进制文件包含的字符串常量
strip               去除二进制文件包含的符号
readelf           显示目标文件详细信息
objdump         尽可能反汇编出源代码
addr2line        根据地址查找代码行
运行阶段
gdb                强大的调试工具
pstack
ldd                 显示程序需要使用的动态库和实际使用的动态库
bpftrace
trace-bpfcc
execsnoop
ftrace
trace-cmd
perf
strace            跟踪程序当前的系统调用
ltrace             跟踪程序当前的库函数
time               查看程序执行时间、用户态时间、内核态时间
gprof              显示用户态各函数执行时间
valgrind          检查内存错误
mtrace           检查内存错误
其他
proc文件系统
系统日志
```

## 第 7 章 设备虚拟化 

设备虚拟化概述
设备虚拟化是虚拟化技术的一个重要方面，它允许虚拟机（VM）访问模拟的外部设备，从而使得虚拟机能够与物理硬件交互，就像是在真实的计算机上运行一样。这在云计算等场景中至关重要，因为虚拟机通常需要访问各种设备，如磁盘、网络接口、图形显示等。

QEMU 的角色与发展历程
传统纯软件模拟
最初，QEMU采用传统的纯软件模拟方式，通过软件实现各种外部设备的模拟。这种方式的优势在于通用性强，可以模拟几乎任何类型的设备，但性能相对较低。

转向 virtio
随着虚拟化技术的发展，QEMU逐渐引入了 virtio 总线。virtio 是一种基于共享内存的设备虚拟化标准，它通过定义一组通用的设备接口和协议，实现了虚拟机与宿主机之间的高效通信。这种方式提高了性能，同时保持了一定的通用性。

转向 VFIO 设备直通
随着硬件对虚拟化的支持不断增强，QEMU 进一步发展，引入了 VFIO（Virtual Function I/O）设备直通技术。VFIO 允许虚拟机直接访问物理设备，绕过 QEMU 的模拟层，提高了性能和效率。

常见的总线系统模拟
QEMU对常见的总线系统进行了模拟，以支持各种设备的虚拟化：

ISA 总线： 用于模拟传统的ISA总线，支持一些古老的设备。
PCI 总线： 模拟了PCI总线，支持更现代的设备，是虚拟机中常见的总线之一。
USB 总线： 用于模拟USB总线及其设备，支持虚拟机中的USB设备连接。
virtio 总线： 为了提高性能，引入了 virtio 总线，用于虚拟机内部的高效通信。
这些总线系统模拟为 QEMU 提供了广泛的设备支持，使得虚拟机能够与各种外部设备进行交互，从而更好地满足不同应用场景的需求。

### 7.4 virtio 设备模拟

virtio 设备模拟概述
virtio 简介

在传统设备模拟中，虚拟机内部设备驱动并不知道自己处于虚拟化环境中，导致 I/O 操作需要经过虚拟机内核栈、QEMU 和宿主机内核栈，产生大量 VM Exit 和 VM Entry，性能较差。virtio 设备模拟方案旨在提高性能，使虚拟机感知到自身处于虚拟化环境中，加载相应的 virtio 总线驱动和 virtio 设备驱动。

半虚拟化的基本原理

半虚拟化的基本原理包括两个主要部分：VMM 创建出模拟设备和操作系统内部安装好该模拟设备的驱动。在半虚拟化环境下，设备和驱动是专门为虚拟化环境设计的，通过自定义接口进行通信，减少 VM Exit 次数，提高性能。

virtio 前后端结构

virtio 采用前后端结构，包括前端驱动（Front-End Driver）、后端设备（Back-End Device）以及自定义传输协议。virtio 不仅适用于 QEMU/KVM，还可以用于其他虚拟化方案。前端驱动负责用户态请求的接收和封装，后端设备接收前端的 I/O 请求，解析数据并完成请求，通过中断机制通知前端。

virtio 数据传输

virtio 前后端驱动的数据传输通过 virtio 队列（virtqueue）实现，一个设备可以注册多个 virtqueue，每个队列处理不同的数据传输，包括控制层面和数据层面的队列。virtqueue 通过 vring 实现，是虚拟机和 QEMU 共享的环形缓冲区。虚拟机将数据描述放入 vring，QEMU 通过读取 vring 获取数据。vring 的基本原理如图 7-21 所示。

virtio vring

virtio 设备模拟通过这种前后端协作结构和数据传输机制，旨在提高虚拟机的 I/O 性能，减少不必要的虚拟化开销。

### 7.5 ioeventfd 和 irqfd 

ioeventfd 和 irqfd
7.5.1 eventfd 原理
eventfd 概述

eventfd 是 Linux 内核提供的一种用于在用户空间和内核空间之间进行事件通知的机制。它提供了一种通过文件描述符传递事件信息的方式。

eventfd 工作原理

创建 eventfd： 用户空间通过 eventfd 系统调用创建一个 eventfd 对象，获得一个文件描述符。

读写操作： 用户空间可以通过读写文件描述符进行事件的等待和通知。读操作可以阻塞，等待事件发生；写操作用于通知事件发生。

内核实现： 内核为每个 eventfd 对象维护一个 64 位的计数器。写操作会增加计数器的值，读操作会减少计数器的值。当计数器的值不为 0 时，读操作可以成功完成，否则读操作可能阻塞。

触发事件： 用户空间通过写操作将计数器值增加，从而通知事件的发生。每次写操作都会唤醒等待中的读操作。

7.5.2 ioeventfd 和 irqfd
ioeventfd 概述

ioeventfd 是一种机制，允许虚拟机通过向 QEMU 注册文件描述符，实现在虚拟机中向指定 I/O 地址写入数据时触发事件。

ioeventfd 工作原理

注册 ioeventfd： 在 QEMU 中，虚拟机可以通过向 KVM 注册 ioeventfd，将一段 I/O 地址范围与一个 eventfd 关联。

触发事件： 当虚拟机写入已注册的 I/O 地址范围时，KVM 收到写操作，触发与该 I/O 地址范围关联的 eventfd，从而通知 QEMU。

QEMU 处理事件： QEMU 主循环中检测到关联的 eventfd 变为可读，执行相应的函数处理事件，绕过了 QEMU 层的分发。

irqfd 概述

irqfd 是一种机制，允许虚拟机通过向 QEMU 注册文件描述符，实现在虚拟机中触发中断。

irqfd 工作原理

注册 irqfd： 虚拟机可以通过向 KVM 注册 irqfd，将一个 eventfd 与一个虚拟中断线索引关联。

触发中断： 当虚拟机写入注册的虚拟中断线索引对应的 I/O 地址时，KVM 收到写操作，触发关联的 eventfd，从而通知 QEMU。

QEMU 处理中断： QEMU 主循环中检测到关联的 eventfd 变为可读，执行相应的函数处理中断，实现了中断的注入。

通过 ioeventfd 和 irqfd 机制，虚拟机可以更高效地与 QEMU 进行通信，减少了 VM Exit 的开销，提高了虚拟机的性能。

### 7.6 vhost net 简介

vhost 概述

vhost 是一种虚拟化技术，用于将虚拟机内部的 I/O 请求直接传递给宿主机内核处理，而不经过用户态的 QEMU，以提高性能。

vhost net 工作原理

前端和后端通信： 虚拟机内部的 virtio 驱动充当前端，负责封装虚拟机内的 I/O 请求到 vring 描述符中。vhost 模块在宿主机内核中作为 virtio 的后端，接收来自虚拟机的通知。

vring 描述符： 虚拟机内的 virtio 驱动通过 vring 描述符传递 I/O 请求信息。这包括需要进行的数据传输的位置、长度等信息。

vhost 处理： vhost 模块在宿主机内核中接收到来自虚拟机的通知后，直接与 tap 设备通信。这绕过了用户态的 QEMU，提高了收发包的性能。

tap 设备： 宿主机的 tap 设备负责与宿主机网络协议栈交互，将数据包发送到真实的网络设备上。

通过使用 vhost net 技术，虚拟机的网络数据包在宿主机内核中的处理路径缩短，避免了用户态和宿主机网络协议栈的多次转换，从而提高了网络性能。vhost net 主要用于优化 virtio 网卡的性能，使其更适用于高性能网络应用场景。

### 7.7 设备直通与 VFIO

VFIO 基本思想与原理

1. 设备直通概述： 设备直通是一种虚拟化技术，将物理设备直接分配给虚拟机，实现虚拟机对设备的直接访问，从而提高性能。

2. 传统设备直通问题： 传统的设备直通方法（如PCI passthrough）要求KVM完成大量工作，包括与IOMMU交互、注册中断处理函数等。这使得KVM过多地涉及与设备的交互，不够通用和灵活。

3. VFIO 的出现： VFIO（Virtual Function I/O）是一种用户态驱动框架，利用硬件层面的I/O虚拟化技术（如Intel的VT-d和AMD的AMD-Vi），将设备直通给虚拟机。

4. VFIO 的基本思想：

资源分解： 将物理设备的各种资源进行分解。
导出接口： 将获取这些资源的接口向上导出到用户空间，使QEMU等应用层软件能够获取硬件的所有资源，包括设备的配置空间、BAR（Base Address Register）空间和中断。

5. VFIO 工作原理： VFIO通过将设备资源导出到用户空间，使QEMU等应用能够直接与这些资源进行交互，包括对设备的配置和中断的处理，而无需KVM过多介入。这提高了设备直通的通用性和灵活性。

VFIO的引入使得虚拟化环境中设备直通更加高效，通过硬件的支持，它提供了一种通用的框架，使得用户态软件能够更灵活地进行设备直通。

#### 聚合与 IOMMU

在VFIO思想的第二部分中，聚合是关键的一环。聚合的目的是将从硬件设备得到的各种资源整合在一起，为虚拟化展示一个完整的设备接口。这个过程在用户空间完成，以QEMU为例，它将硬件设备分解得到的资源重新整合成虚拟机设备，并将其挂载到虚拟机上。QEMU通过调用KVM的接口将这些资源与虚拟机关联，使虚拟机内部能够无感知VFIO的存在，正常与直通设备进行交互。

在虚拟化环境中，将设备直通给虚拟机带来两个挑战：设备DMA使用的地址和中断的重定向。虚拟机内部可以随意指定设备DMA地址，因此需要一种机制来隔离对设备DMA地址的访问。此外，在Intel架构上，MSI（Message Signaled Interrupt）中断通过写入地址完成，任何DMA发起者都能够写入任意数据，可能导致虚拟机内部攻击者产生不属于其的中断。

解决这些问题的关键是IOMMU（Input-Output Memory Management Unit）。IOMMU的主要功能是DMA重映射，确保设备的DMA访问仅限于由宿主机分配的内存。类似于MMU将CPU访问的虚拟机地址转换为物理地址，IOMMU对设备的DMA地址进行重映射。

IOMMU的工作原理包括DMA Remapping和Interrupt Remapping。DMA Remapping通过建立类似页表的结构来完成DMA地址的转换，确保访问仅限于被分配的物理地址。Interrupt Remapping通过IOMMU对中断请求进行重定向，将直通设备的内部中断正确分派到虚拟机。这一流程确保了设备直通的安全性和正确性。

#### VFIO 框架设计

VFIO框架设计简洁清晰，主要包含以下组件：

VFIO Interface: 作为接口层，向应用层导出一系列接口。用户程序如QEMU通过相应的ioctl与VFIO进行交互。

iommu driver: 物理硬件IOMMU的驱动实现，例如Intel和AMD的IOMMU。

pci_bus driver: 物理PCI设备的驱动程序。

vfio_iommu: 对底层iommu driver的封装，向上提供IOMMU功能，包括DMA Remapping和Interrupt Remapping。

vfio_pci: 对设备驱动的封装，向用户进程提供访问设备驱动的功能，包括配置空间和模拟BAR。

重要的VFIO功能之一是对各个设备进行分区。在VFIO中，有三个关键的概念：container、group和device。它们的关系如图7-55所示。

#### VFIO 关键概念解析

Group: Group是IOMMU能进行DMA隔离的最小单元。在一个Group内可能包含一个或多个设备，这取决于硬件IOMMU的拓扑结构。在设备直通时，一个Group内的设备必须一起直通给一个虚拟机。这是为了确保物理上的DMA隔离，防止虚拟机中的设备攻击其他虚拟机或宿主机。

Device: Device指的是要操作的硬件设备。在IOMMU拓扑中，如果设备是独立的，它自身就构成一个IOMMU Group。对于多功能设备（multi-function device），所有的function一起构成一个IOMMU Group，因为它们在物理硬件上是互联的，需要进行隔离。

Container: Container是由多个Group组成的集合。虽然Group是VFIO的最小隔离单元，但有时候并不是最佳的分割粒度。多个Group可以共享一组页表，将它们组织到一个Container中，可以提高系统性能，也更方便用户管理。通常，每个进程或虚拟机可以作为一个Container。

#### 2.VFIO 使用方法

```text
上面介绍了 VFIO 的基本原理，下面介绍 VFIO 的使用方法。

1）假设需要直通的设备如下所示。
01:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function 
(rev 01) 

2）找到这个设备的 VFIO group，这是由内核生成的。
# readlink /sys/bus/pci/devices/0000:01:10.0/iommu_group 
../../../../kernel/iommu_groups/15 

3）查看 group 里面的设备，这个 group 只有一个设备。
# ls /sys/bus/pci/devices/0000:01:10.0/iommu_group/devices/ 
0000:01:10.0 

4）将设备与驱动程序解绑。
# echo 0000:01:10.0 >/sys/bus/pci/devices/0000:01:10.0/driver/unbind 

5）找到设备的生产商&设备 ID。
$ lspci -n -s 01:10.0 
01:10.0 0200: 8086:10ca (rev 01) 

6）将设备绑定到 vfio-pci 驱动，这会导致一个新的设备节点“/dev/vfio/15”被创建，这个
节点表示直通设备所属的 group 文件，用户态程序可以通过该节点操作直通设备的 group。
$ echo 8086 10ca /sys/bus/pci/drivers/vfio-pci/new_id 

7）修改这个设备节点的属性。
# chownqemu /dev/vfio/15 
# chgrpqemu /dev/vfio/15 

8）设置能够锁定的内存为虚拟机内存+一些 IO 空间。
# ulimit -l 2621440 # ((2048 + 512) ＊ 1024) 

9）向 QEMU 传递相关参数。
sudo qemuqemu-system-x86_64 -m 2048 -hda rhel6vm \ 
 -vga std -vnc :0 -net none \ 
 -enable-kvm \ 
 -device vfio-pci,host=01:10.0,id=net0
```
